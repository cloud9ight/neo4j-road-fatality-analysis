{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495084df",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "create a filtered subset of the data to improve performance and obtain results for pathfinding. other etl logic same as etl_process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74d389",
   "metadata": {},
   "source": [
    "## Cell 1: Setup for filtering and Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "259bf46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 10490 rows from the full dataset: ../data/source/Project2_Dataset_Corrected.csv\n",
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "SOURCE_FULL_DATASET_FILE  = os.path.join('..','data', 'source', 'Project2_Dataset_Corrected.csv') \n",
    "OUTPUT_DIR_FILTERED = os.path.join('..','data', 'import_filtered')\n",
    "\n",
    "FILTER_STATE_VALUE = 'NSW'\n",
    "FILTER_YEAR_VALUE = 2024\n",
    "\n",
    "# --- Load the FULL original dataset ---\n",
    "df_raw = pd.read_csv(SOURCE_FULL_DATASET_FILE, low_memory=False)\n",
    "print(f\"Successfully loaded {len(df_raw)} rows from the full dataset: {SOURCE_FULL_DATASET_FILE}\")\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd2d3b",
   "metadata": {},
   "source": [
    "## Cell 2: Apply state&year Filter and Clean Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6371cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying filter: State == 'NSW' AND Year == '2024'\n",
      "Filtered dataset for 'NSW' and Year 2024 contains 339 rows.\n",
      "Column names have been cleaned for the filtered DataFrame.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 339 entries, 0 to 978\n",
      "Data columns (total 25 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   id                             339 non-null    int64 \n",
      " 1   crash_id                       339 non-null    int64 \n",
      " 2   state                          339 non-null    object\n",
      " 3   month                          339 non-null    int64 \n",
      " 4   year                           339 non-null    int64 \n",
      " 5   dayweek                        339 non-null    object\n",
      " 6   time                           339 non-null    object\n",
      " 7   crash_type                     339 non-null    object\n",
      " 8   number_fatalities              339 non-null    int64 \n",
      " 9   bus_involvement                339 non-null    object\n",
      " 10  heavy_rigid_truck_involvement  339 non-null    object\n",
      " 11  articulated_truck_involvement  339 non-null    object\n",
      " 12  speed_limit                    339 non-null    int64 \n",
      " 13  road_user                      339 non-null    object\n",
      " 14  gender                         339 non-null    object\n",
      " 15  age                            339 non-null    int64 \n",
      " 16  national_remoteness_areas      339 non-null    object\n",
      " 17  sa4_name_2021                  339 non-null    object\n",
      " 18  national_lga_name_2024         339 non-null    object\n",
      " 19  national_road_type             339 non-null    object\n",
      " 20  christmas_period               339 non-null    object\n",
      " 21  easter_period                  339 non-null    object\n",
      " 22  age_group                      339 non-null    object\n",
      " 23  day_of_week                    339 non-null    object\n",
      " 24  time_of_day                    339 non-null    object\n",
      "dtypes: int64(7), object(18)\n",
      "memory usage: 68.9+ KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Applying filter: State == '{FILTER_STATE_VALUE}' AND Year == '{FILTER_YEAR_VALUE}'\")\n",
    "\n",
    "df_filtered = df_raw[\n",
    "    (df_raw['State'] == FILTER_STATE_VALUE) &\n",
    "    (df_raw['Year'] == FILTER_YEAR_VALUE) \n",
    "].copy()\n",
    "print(f\"Filtered dataset for '{FILTER_STATE_VALUE}' and Year 2024 contains {len(df_filtered)} rows.\")\n",
    "\n",
    "def clean_col_name(col_name):\n",
    "    col_name = str(col_name)\n",
    "    col_name = col_name.replace(' (', '_').replace(') ', '_').replace(' ', '_')\n",
    "    clean_name = re.sub(r'[^a-zA-Z0-9_]+', '', col_name).lower()\n",
    "    return clean_name.strip('_')\n",
    "\n",
    "df_filtered.columns = [clean_col_name(col) for col in df_filtered.columns]\n",
    "print(\"Column names have been cleaned for the filtered DataFrame.\")\n",
    "\n",
    "df = df_filtered.copy() # Use 'df' for consistency in subsequent cells\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66916c",
   "metadata": {},
   "source": [
    "## ----- from NSW2024 filtered df -----\n",
    "## Cell 3: Create Location Nodes (State, SA4, LGA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "625b4763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Location Node Files (from NSW2024 data) ---\n",
      "Saved 1 State nodes.\n",
      "Saved 28 SA4 nodes.\n",
      "Saved 97 LGA nodes.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creating Location Node Files (from NSW2024 data) ---\")\n",
    "\n",
    "states_nodes = df[['state']].drop_duplicates()\n",
    "states_nodes.rename(columns={'state': 'name:ID(State)'}, inplace=True)\n",
    "states_nodes[':LABEL'] = 'State'\n",
    "states_nodes.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'states.csv'), index=False, encoding='utf-8')\n",
    "print(f\"Saved {len(states_nodes)} State nodes.\")\n",
    "\n",
    "sa4_link_data = df[['sa4_name_2021', 'state']].drop_duplicates()\n",
    "sa4_nodes = sa4_link_data[['sa4_name_2021']].drop_duplicates().copy()\n",
    "sa4_nodes.rename(columns={'sa4_name_2021': 'name:ID(SA4)'}, inplace=True)\n",
    "sa4_nodes[':LABEL'] = 'SA4'\n",
    "sa4_nodes.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'sa4s.csv'), index=False, encoding='utf-8')\n",
    "print(f\"Saved {len(sa4_nodes)} SA4 nodes.\")\n",
    "\n",
    "lga_link_data = df[['national_lga_name_2024', 'sa4_name_2021']].drop_duplicates()\n",
    "lga_nodes = lga_link_data[['national_lga_name_2024']].drop_duplicates().copy()\n",
    "lga_nodes.rename(columns={'national_lga_name_2024': 'name:ID(LGA)'}, inplace=True)\n",
    "lga_nodes[':LABEL'] = 'LGA'\n",
    "lga_nodes.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'lgas.csv'), index=False, encoding='utf-8')\n",
    "print(f\"Saved {len(lga_nodes)} LGA nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4513790",
   "metadata": {},
   "source": [
    "## Cell 4: Create Unique Crash Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fbc4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Unique Crash Node File (from NSW2024 data) ---\n",
      "Saved 307 unique Crash nodes.\n",
      "Created crash_id to internalCrashID map (from NSW data).\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creating Unique Crash Node File (from NSW2024 data) ---\")\n",
    "crash_data = df.drop_duplicates(subset=['crash_id'], keep='first').reset_index(drop=True).copy()\n",
    "crash_data['internalCrashID:ID(Crash)'] = crash_data.index\n",
    "\n",
    "yes_no_cols = ['bus_involvement', 'heavy_rigid_truck_involvement', 'articulated_truck_involvement', 'christmas_period', 'easter_period']\n",
    "for col in yes_no_cols:\n",
    "    if col in crash_data.columns:\n",
    "        crash_data[col] = crash_data[col].map({'Yes': 'yes', 'No': 'no'})\n",
    "\n",
    "crash_data['year'] = crash_data['year'].astype(int)\n",
    "crash_data['month'] = crash_data['month'].astype(int)\n",
    "crash_data['number_fatalities'] = crash_data['number_fatalities'].astype(int)\n",
    "crash_data['speed_limit'] = pd.to_numeric(crash_data['speed_limit'], errors='coerce').astype('Int64')\n",
    "\n",
    "crash_node_cols = {\n",
    "    'internalCrashID:ID(Crash)': 'internalCrashID:ID(Crash)', 'crash_id': 'crashID_orig',\n",
    "    'year': 'year', 'month': 'month', 'dayweek': 'dayweek', 'time': 'time',\n",
    "    'crash_type': 'crashType', 'number_fatalities': 'numberFatalities', 'bus_involvement': 'busInvolvement',\n",
    "    'heavy_rigid_truck_involvement': 'heavyRigidTruckInvolvement', 'articulated_truck_involvement': 'articulatedTruckInvolvement',\n",
    "    'speed_limit': 'speedLimit', 'national_road_type': 'nationalRoadType', 'christmas_period': 'christmasPeriod',\n",
    "    'easter_period': 'easterPeriod', 'national_remoteness_areas': 'nationalRemotenessAreas',\n",
    "    'day_of_week': 'dayOfWeekType', 'time_of_day': 'timeOfDay'\n",
    "}\n",
    "existing_cols = [col for col in crash_node_cols.keys() if col in crash_data.columns]\n",
    "crash_nodes_df = crash_data[existing_cols].copy()\n",
    "rename_map_crash = {k: v for k, v in crash_node_cols.items() if k in existing_cols}\n",
    "crash_nodes_df.rename(columns=rename_map_crash, inplace=True)\n",
    "crash_nodes_df[':LABEL'] = 'Crash'\n",
    "crash_nodes_df.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'crashes.csv'), index=False, encoding='utf-8')\n",
    "print(f\"Saved {len(crash_nodes_df)} unique Crash nodes.\")\n",
    "\n",
    "crash_id_map = crash_data[['crash_id', 'internalCrashID:ID(Crash)']].copy().set_index('crash_id')\n",
    "print(\"Created crash_id to internalCrashID map (from NSW2024 data).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732656b2",
   "metadata": {},
   "source": [
    "## Cell 5: Create Person Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ee25f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Person Node File (from NSW2024 data) ---\n",
      "Saved 339 Person nodes.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creating Person Node File (from NSW2024 data) ---\")\n",
    "df['age'] = df['age'].astype(int) \n",
    "\n",
    "person_node_cols = {\n",
    "    'id': 'personID:ID(Person)',\n",
    "    'road_user': 'roadUser', 'gender': 'gender', 'age': 'age', 'age_group': 'ageGroup'\n",
    "}\n",
    "existing_cols = [col for col in person_node_cols.keys() if col in df.columns]\n",
    "person_nodes_df = df[existing_cols].copy()\n",
    "rename_map_person = {k: v for k, v in person_node_cols.items() if k in existing_cols}\n",
    "person_nodes_df.rename(columns=rename_map_person, inplace=True)\n",
    "person_nodes_df[':LABEL'] = 'Person'\n",
    "person_nodes_df.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'persons.csv'), index=False, encoding='utf-8')\n",
    "print(f\"Saved {len(person_nodes_df)} Person nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e7236",
   "metadata": {},
   "source": [
    "## Cell 6: Create Relationship Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fd4cd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating Relationship Files (from NSW2024 data) ---\n",
      "Saved 28 SA4->State relationships.\n",
      "Saved 307 Crash->LGA relationships.\n",
      "Saved 307 Crash->SA4 relationships.\n",
      "Saved 339 Person->Crash relationships.\n",
      "\n",
      "--- All Relationship File Generation Complete (for NSW2024 filtered data) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Creating Relationship Files (from NSW2024 data) ---\")\n",
    "\n",
    "# --- 1. SA4 -> State ---\n",
    "try:\n",
    "    sa4_state_rels = sa4_link_data.copy()\n",
    "    sa4_state_rels.rename(columns={'sa4_name_2021': ':START_ID(SA4)', 'state': ':END_ID(State)'}, inplace=True)\n",
    "    sa4_state_rels[':TYPE'] = 'IN_STATE'\n",
    "    sa4_state_rels.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'rels_sa4_state.csv'), index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(sa4_state_rels)} SA4->State relationships.\")\n",
    "except Exception as e: print(f\"Error in SA4->State: {e}\")\n",
    "\n",
    "# --- 2. Crash -> LGA  ---\n",
    "try:\n",
    "    if 'internalCrashID:ID(Crash)' in crash_data.columns and 'national_lga_name_2024' in crash_data.columns:\n",
    "        crash_lga_rels = crash_data[['internalCrashID:ID(Crash)', 'national_lga_name_2024']].copy()\n",
    "        crash_lga_rels.rename(columns={'internalCrashID:ID(Crash)': ':START_ID(Crash)', 'national_lga_name_2024': ':END_ID(LGA)'}, inplace=True)\n",
    "        crash_lga_rels[':TYPE'] = 'OCCURRED_IN_LGA'\n",
    "        crash_lga_rels.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'rels_crash_lga.csv'), index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(crash_lga_rels)} Crash->LGA relationships.\")\n",
    "    else:\n",
    "        print(\"Error or skipping Crash->LGA: Source columns not found in crash_data.\")\n",
    "except Exception as e: print(f\"Error in Crash->LGA: {e}\")\n",
    "\n",
    "# --- 3. Crash -> SA4  ---\n",
    "try:\n",
    "    if 'internalCrashID:ID(Crash)' in crash_data.columns and 'sa4_name_2021' in crash_data.columns:\n",
    "        crash_sa4_rels = crash_data[['internalCrashID:ID(Crash)', 'sa4_name_2021']].copy()\n",
    "        crash_sa4_rels.rename(columns={'internalCrashID:ID(Crash)': ':START_ID(Crash)', 'sa4_name_2021': ':END_ID(SA4)'}, inplace=True)\n",
    "        crash_sa4_rels[':TYPE'] = 'OCCURRED_IN_SA4'\n",
    "        crash_sa4_rels.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'rels_crash_sa4.csv'), index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(crash_sa4_rels)} Crash->SA4 relationships.\")\n",
    "    else:\n",
    "        print(\"Error or skipping Crash->SA4: Source columns not found.\")\n",
    "except Exception as e: print(f\"Error in Crash->SA4: {e}\")\n",
    "\n",
    "# --- 4. Person -> Crash ---\n",
    "try:\n",
    "    df_merged_person = df.merge(crash_id_map, left_on='crash_id', right_index=True, how='inner')\n",
    "    if 'id' in df_merged_person.columns and 'internalCrashID:ID(Crash)' in df_merged_person.columns:\n",
    "        person_crash_rels = df_merged_person[['id', 'internalCrashID:ID(Crash)']].copy()\n",
    "        person_crash_rels.rename(columns={'id': ':START_ID(Person)', 'internalCrashID:ID(Crash)': ':END_ID(Crash)'}, inplace=True)\n",
    "        person_crash_rels[':TYPE'] = 'WAS_INVOLVED_IN'\n",
    "        person_crash_rels.to_csv(os.path.join(OUTPUT_DIR_FILTERED, 'rels_person_crash.csv'), index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(person_crash_rels)} Person->Crash relationships.\")\n",
    "    else:\n",
    "        print(\"Error or skipping Person->Crash: Required columns not found after merge.\")\n",
    "except Exception as e: print(f\"Error in Person->Crash: {e}\")\n",
    "\n",
    "print(\"\\n--- All Relationship File Generation Complete (for NSW2024 filtered data) ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
